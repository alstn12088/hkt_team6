{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5db90716-36df-443a-b922-519da74ddcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import exp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5df157-133f-4fdd-9442-487ead076c99",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "다섯개의 서로 다른 종류의 데이터가 존재한다. (data_index 1~5)\n",
    "\n",
    "features는 작업자에게 추천해야 하는 카테고리이다. \n",
    "\n",
    "contrains는 작업자의 추천 변수의 추천 값이며 +- 2%의 값 안에 작업자 추천 변수 값이 부여되면 좋다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814bfe5-a099-4f03-8d15-d3eee9315d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your list of column names\n",
    "df_feat = pd.read_csv('hkt_data/features_new.csv', encoding='euc-kr')\n",
    "\n",
    "data_index = 1\n",
    "\n",
    "features = df_feat.to_numpy()[:,0]\n",
    "constrains = df_feat.to_numpy()[:,data_index]\n",
    "targets = ['gt_LWM_STD_WGT', 'gt_MRM_WGT','gt_STD_WGT','gt_UPM_STD_WGT']\n",
    "\n",
    "df_data = pd.read_csv('hkt_data/2301_2306_data_{}.csv'.format(data_index), encoding='euc-kr')\n",
    "df = df_data[features]\n",
    "df_targets = df_data[targets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f713b3-4acf-44b8-9f32-b83a66a940d2",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "주어진 변수를 Normalize 해야한다. \n",
    "\n",
    "가장 좋은 방법은 maximum 값과 minimum 값을 기준으로 min-max scaling을 통해 모든 값을 [0,1]로 매핑하는 것이다. \n",
    "\n",
    "Constraint 가 중요한 작업에서는 주어진 lower bound와 upperbound를 기준으로 normalization을 해도 좋다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9788e14a-aef8-438d-84fd-1dc78126c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize columns using min-max scaling\n",
    "\n",
    "\n",
    "lower_bound = constrains - constrains * 0.02\n",
    "upper_bound = constrains + constrains * 0.02\n",
    "\n",
    "for i in range(len(constrains)):\n",
    "    if math.isnan(constrains[i]):\n",
    "        lower_bound[i] = df.min()[features[i]]\n",
    "        upper_bound[i] = df.max()[features[i]]\n",
    "\n",
    "\n",
    "# df_x = (df - lower_bound ) / (upper_bound - lower_bound)\n",
    "df_x = (df - df.min() ) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f26ae0-8e79-44cd-b377-588de8696daf",
   "metadata": {},
   "source": [
    "# Score Calibration\n",
    "\n",
    "Score 값을 정해 주어야 한다. 우리는 간단하게 gt_MRM_WGT를 score y로 정의하였다. \n",
    "\n",
    "우리는 원하는 target 인 gt_STD_WGT에 대해서 (i.e. y*), 역 생성 모델 p(x|y=y*) 를 통해 x를 생성할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb5be101-2581-4b05-b24d-47ffe7e79099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "temp = 0.3\n",
    "\n",
    "a = 'gt_LWM_STD_WGT'\n",
    "b = 'gt_UPM_STD_WGT'\n",
    "x = 'gt_MRM_WGT'\n",
    "x_star = 'gt_STD_WGT'\n",
    "\n",
    "\n",
    "def calculate_score(row):\n",
    "    score = row[x]\n",
    "    return score\n",
    "\n",
    "# Apply the scoring function to create the 'Y' column\n",
    "df_y= df_targets.apply(calculate_score, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc343f-c717-4f22-a331-9b6863dd3885",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "일단 데이터는 시계열 특성을 가지고 있고, 이는 잘 정렬되어 있으므로, index 순서대로 threshold 지점으로 끊으면, 과거 데이터가 training data, \n",
    "그 이후의 데이터가 validation data가 된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "631457cd-2629-4ff1-8735-d6adf11aa4dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "threshold = int(len(df_x) * 0.7)\n",
    "\n",
    "x_train = df_x[:threshold]\n",
    "y_train = df_y[:threshold]\n",
    "\n",
    "x_valid = df_x[threshold:]\n",
    "y_valid = df_y[threshold:]\n",
    "\n",
    "\n",
    "# Convert pandas DataFrames to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "x_valid_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "# x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Define custom Dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 256\n",
    "train_dataset = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = CustomDataset(x_valid_tensor, y_valid_tensor)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679bf0b-c50c-4dcf-939a-94d59c1861d5",
   "metadata": {},
   "source": [
    "# Oracle Model Training\n",
    "\n",
    "Oracle 이라 함은 완벽한 model을 뜻한다. 실전 상황에서 사용 할 수 있는 모델 (예를 들어 공장 그 자체가 oracle 일 수 있음)을 뜻하며, 학습 과정에서는 사용해서는 안되고 오로지 validation 용으로만 사용 가능하다. Oracle 로 가장 좋은 것은 공장 자체가 modelling 되는 것이지만 현실적으로 불가능하다. 따라서 시계열 데이터의 모든 데이터를 다 주고, 이를 학습한 ML 모델을 Oracle이라고 가정한다 [1] 우리의 역생성 모델은 Oracle을 참고하지 않고 학습되어야 한다. \n",
    "\n",
    "[1] Trabucco, Brandon, et al. \"Design-bench: Benchmarks for data-driven offline model-based optimization.\" International Conference on Machine Learning. PMLR, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "edc4e7fc-a978-49ec-a682-08d4901d2208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle Regression Validation MSE: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# Oracle Score Function\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Random Forest Regression model for Oracle Training\n",
    "oracle_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Oracle Can See Every Data\n",
    "oracle_model.fit(df_x, df_y)\n",
    "oracle_preds = oracle_model.predict(x_valid)\n",
    "oracle_mse = mean_squared_error(y_valid, oracle_preds)\n",
    "\n",
    "print(f\"Oracle Regression Validation MSE: {oracle_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9f69e-16cb-49a5-88ee-6d5704d9dadf",
   "metadata": {},
   "source": [
    "# Proxy Model Training\n",
    "\n",
    "Proxy란 대체자를 뜻하는 단어이다. Proxy model은 Oracle model 과는 다르게 접근이 가능하다. Proxy model은 당연히 모든 데이터로 학습 하면 안되고, training data로만 학습되어야 한다. Proxy model은 불완전한 model 이고, 종종 큰 오류를 야기할 수 있다. 예를들어, Proxy model로만 의존해서 의사 결정을 하는 것은 Adversarial sample 등의 문제를 야기할 수 있다 [2]. 따라서 참고로만 사용해야 하는 모델이다. \n",
    "\n",
    "\n",
    "[2] Trabucco, Brandon, et al. \"Conservative objective models for effective offline model-based optimization.\" International Conference on Machine Learning. PMLR, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "664bb3ad-e04a-4c93-a813-5adef4b17380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy Regression Validation MSE: 0.0392\n"
     ]
    }
   ],
   "source": [
    "# Proxy Score Function\n",
    "\n",
    "# Random Forest Regression model\n",
    "proxy_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Proxy only see part of data\n",
    "proxy_model.fit(x_train, y_train)\n",
    "proxy_preds = proxy_model.predict(x_valid)\n",
    "proxy_mse = mean_squared_error(y_valid, proxy_preds)\n",
    "\n",
    "print(f\"Proxy Regression Validation MSE: {proxy_mse:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb66c1-79dc-4ffc-81cc-aa16b3d7f87e",
   "metadata": {},
   "source": [
    "# Conditional Variational AutoEncoder (CVAE)\n",
    "\n",
    "CVAE는 Genertive model 로서 condition variable $y$ 에 대해서 decision or design variable $x$ 를 generate하는 확률 모델이다. \n",
    "\n",
    "즉 CVAE는 $p(x|y,z)$ 를 모델하게 된다. \n",
    "\n",
    "구체적은 학습 과정으로는:\n",
    "\n",
    "1. Encoder 는 $x$와 $y$ 로 부터 평균과 표준편차를 예측하고,\n",
    "2. Decoder는 Encoder의 평균과 표준편차와 $y$ 값을 input으로 받아서 output으로 $x$를 reconstruct 한다. \n",
    "\n",
    "학습 후 사용할 때는 Decoder만 사용하게 되고, 우리가 원하는 정규분포에서 랜덤하게 샘플한 평균, 표준편자, 그리고 원하는 $y*$ 값을 넣어주면, $x$가 생성된다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "955c2a43-4b35-4efc-8a9f-191784952594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cVAE model\n",
    "class cVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, condition_dim):\n",
    "        super(cVAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, condition):\n",
    "        input_condition = torch.cat((x, condition), dim=1)\n",
    "        hidden = self.encoder(input_condition)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, condition):\n",
    "        latent_condition = torch.cat((z, condition), dim=1)\n",
    "        reconstructed = self.decoder(latent_condition)\n",
    "        return reconstructed\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mu, logvar = self.encode(x, condition)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z, condition)\n",
    "        return reconstructed, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baaecd9-2135-4cd0-93f4-bc77a6dd1bf1",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "input_dim 은 말 그대로 input 되는 변수의 dimension 으로, 우리에게는 feature의 개수가 되겠다. \n",
    "\n",
    "hidden_dim 은 encoder와 decoder에서 representation learning 을 할 때의 MLP의 hidden dimension이다. \n",
    "\n",
    "lagent_dim 은 conditioning code와 합쳐지기 전의 representation dimension이다. \n",
    "\n",
    "condition_dim 은 input condition의 dimension으로 우리는 어떠한 스칼라 값 $y$을 condition으로 주기 때문에 1차원이다.  \n",
    "\n",
    "optimizer는 adam optimizer를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "49b8fdc6-8b2d-49aa-a132-d3843989b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = len(features)\n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "condition_dim = 1  # Scalar condition\n",
    "\n",
    "# Create the cVAE model\n",
    "vae = cVAE(input_dim, hidden_dim, latent_dim, condition_dim)\n",
    "\n",
    "# Define loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f4866-cb15-43d8-b307-889a6e2340cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training CVAE\n",
    "\n",
    "전형적인 Pytorch training code이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "26de7a61-bc42-4dc6-aaab-e89c8333479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average loss: 6.6852\n",
      "Epoch 2: Average loss: 5.5476\n",
      "Epoch 3: Average loss: 5.5453\n",
      "Epoch 4: Average loss: 5.5434\n",
      "Epoch 5: Average loss: 5.5424\n",
      "Epoch 6: Average loss: 5.5418\n",
      "Epoch 7: Average loss: 5.5414\n",
      "Epoch 8: Average loss: 5.5411\n",
      "Epoch 9: Average loss: 5.5410\n",
      "Epoch 10: Average loss: 5.5407\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Example training loop using your dataset and data loaders\n",
    "def train_vae(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, condition) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(-1, input_dim)  # Reshape data if needed\n",
    "        condition = condition.view(-1, condition_dim)  # Reshape condition if needed\n",
    "        \n",
    "        recon_batch, mu, logvar = vae(data, condition)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch {}: Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "# Example usage with your dataset and data loaders\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_vae(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42565392-a738-4eb1-a7df-4656e4d81e3f",
   "metadata": {},
   "source": [
    "# Decoding after training\n",
    "\n",
    "일전에 언급했던 것 같이 결국 학습된 CVAE에서 사용되는 부분은 학습된 디코더이다. \n",
    "\n",
    "디코더에게 우리가 원하는 condition_value를 input으로 넣어주면 $x$가 sample 된다. \n",
    "\n",
    "condition_value는 자유롭게 정할 수 있다. \n",
    "\n",
    "num_sample은 sample의 개수이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "045b2b30-a78a-49bd-a124-35d742c427cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6318, 0.3343, 0.4549, 0.6203, 0.0538, 0.0567, 0.5732, 0.2402, 0.0658,\n",
      "         0.1409, 0.3677],\n",
      "        [0.6249, 0.3312, 0.4569, 0.6068, 0.0533, 0.0525, 0.5658, 0.2359, 0.0689,\n",
      "         0.1421, 0.3721],\n",
      "        [0.6414, 0.3287, 0.4477, 0.6111, 0.0557, 0.0619, 0.5632, 0.2431, 0.0738,\n",
      "         0.1486, 0.3631],\n",
      "        [0.6223, 0.3271, 0.4436, 0.6202, 0.0540, 0.0561, 0.5798, 0.2405, 0.0667,\n",
      "         0.1456, 0.3596],\n",
      "        [0.6265, 0.3367, 0.4591, 0.6124, 0.0548, 0.0577, 0.5596, 0.2376, 0.0731,\n",
      "         0.1452, 0.3597],\n",
      "        [0.6329, 0.3364, 0.4546, 0.6187, 0.0538, 0.0567, 0.5793, 0.2339, 0.0757,\n",
      "         0.1387, 0.3564],\n",
      "        [0.6288, 0.3307, 0.4493, 0.6105, 0.0541, 0.0568, 0.5717, 0.2415, 0.0685,\n",
      "         0.1348, 0.3640],\n",
      "        [0.6194, 0.3344, 0.4489, 0.6205, 0.0566, 0.0552, 0.5739, 0.2306, 0.0714,\n",
      "         0.1405, 0.3602],\n",
      "        [0.6379, 0.3317, 0.4595, 0.6213, 0.0608, 0.0587, 0.5730, 0.2366, 0.0696,\n",
      "         0.1455, 0.3672],\n",
      "        [0.6277, 0.3272, 0.4547, 0.6127, 0.0492, 0.0516, 0.5835, 0.2223, 0.0653,\n",
      "         0.1330, 0.3701],\n",
      "        [0.6343, 0.3372, 0.4539, 0.6082, 0.0482, 0.0554, 0.5718, 0.2313, 0.0629,\n",
      "         0.1393, 0.3530],\n",
      "        [0.6251, 0.3326, 0.4641, 0.6194, 0.0535, 0.0553, 0.5635, 0.2316, 0.0693,\n",
      "         0.1363, 0.3619],\n",
      "        [0.6310, 0.3357, 0.4549, 0.6045, 0.0534, 0.0578, 0.5744, 0.2334, 0.0710,\n",
      "         0.1349, 0.3460],\n",
      "        [0.6176, 0.3305, 0.4521, 0.6183, 0.0559, 0.0538, 0.5788, 0.2425, 0.0690,\n",
      "         0.1431, 0.3720],\n",
      "        [0.6305, 0.3286, 0.4491, 0.6057, 0.0554, 0.0551, 0.5733, 0.2441, 0.0731,\n",
      "         0.1411, 0.3643]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Example function to decode using trained cVAE with a specific condition value\n",
    "def decode_with_condition(condition_value, num_samples=1):\n",
    "    vae.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        condition = torch.tensor([[condition_value]] * num_samples, dtype=torch.float32)\n",
    "        condition = condition.to(device)\n",
    "        \n",
    "        z_samples = torch.randn(num_samples, latent_dim).to(device)  # Sample random latent vectors\n",
    "        \n",
    "        reconstructed_samples = vae.decode(z_samples, condition)\n",
    "    return reconstructed_samples\n",
    "\n",
    "# Example usage of the decode function\n",
    "condition_value = 11.8  # The condition value you want to query\n",
    "num_samples = 15  # Number of samples to generate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "decoded_samples = decode_with_condition(condition_value, num_samples)\n",
    "\n",
    "# Print or visualize the decoded samples as needed\n",
    "print(decoded_samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f848efe-e2f9-4c8f-bee3-5437a2e87672",
   "metadata": {},
   "source": [
    "# Oracle Test\n",
    "\n",
    "우리의 역생성모델이 만든 $x$가 얼마나 잘 만들어졌는지 검증하기 위해서 oracle model을 마지막으로 이용하여 해당 값을 유추한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1ead5e6a-f834-4255-a368-914fc496cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.163513333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as12088/anaconda3/envs/hkt/lib/python3.9/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    decoded_samples = decoded_samples.cpu()\n",
    "\n",
    "print(oracle_model.predict(decoded_samples).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053044f6-9263-486c-b7dd-8756561bcfaf",
   "metadata": {},
   "source": [
    "# Renormalization\n",
    "\n",
    "다시 원래의 데이터 형태로 돌리기 위해서 renormalization 을 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e51998ea-9d44-4ab2-93bb-7b389f5624af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BUILDING_Tread 폭 - 실측  BUILDING_Tread 길이 - 실측  BUILDING_Belt #1 - 폭   \\\n",
      "0              260.484223             2007.837882             221.641014   \n",
      "1              260.177878             2007.599909             221.674440   \n",
      "2              260.907645             2007.410595             221.519968   \n",
      "3              260.062075             2007.290422             221.450971   \n",
      "4              260.247607             2008.026912             221.710645   \n",
      "5              260.531129             2008.003160             221.634859   \n",
      "6              260.352236             2007.568202             221.546602   \n",
      "7              259.936145             2007.849069             221.538705   \n",
      "8              260.755020             2007.641769             221.718168   \n",
      "9              260.303081             2007.299259             221.637693   \n",
      "10             260.595668             2008.066134             221.623051   \n",
      "11             260.188201             2007.707588             221.794708   \n",
      "12             260.450572             2007.947907             221.639811   \n",
      "13             259.854031             2007.549996             221.593598   \n",
      "14             260.426630             2007.403526             221.543409   \n",
      "\n",
      "    BUILDING_Belt #2 - 폭   BUILDING_Carcass #1 - 폭   BUILDING_Carcass #2 - 폭   \\\n",
      "0              208.602450                531.578453                441.877087   \n",
      "1              208.456695                531.568152                441.791141   \n",
      "2              208.502875                531.617318                441.983644   \n",
      "3              208.601213                531.583039                441.864207   \n",
      "4              208.517511                531.598651                441.897057   \n",
      "5              208.584365                531.577071                441.877895   \n",
      "6              208.496580                531.585114                441.878270   \n",
      "7              208.603986                531.637884                441.846099   \n",
      "8              208.613086                531.725424                441.917246   \n",
      "9              208.520190                531.480806                441.771773   \n",
      "10             208.472228                531.459539                441.850150   \n",
      "11             208.592392                531.572225                441.848871   \n",
      "12             208.431796                531.569562                441.899806   \n",
      "13             208.580462                531.621804                441.817014   \n",
      "14             208.445355                531.612592                441.844610   \n",
      "\n",
      "    BUILDING_Inner - 폭   BUILDING_PA - 폭   BUILDING_Belt #1 - 길이  \\\n",
      "0            423.417062        600.399650            1966.604953   \n",
      "1            423.113333        600.244092            1967.564380   \n",
      "2            423.006568        600.503590            1969.120846   \n",
      "3            423.688037        600.410921            1966.895694   \n",
      "4            422.856924        600.303191            1968.899781   \n",
      "5            423.669769        600.171296            1969.705792   \n",
      "6            423.354895        600.444701            1967.447696   \n",
      "7            423.444000        600.049882            1968.357029   \n",
      "8            423.407253        600.268436            1967.790601   \n",
      "9            423.840739        599.751425            1966.441723   \n",
      "10           423.357664        600.077620            1965.678593   \n",
      "11           423.016335        600.087142            1967.694908   \n",
      "12           423.464635        600.154377            1968.224739   \n",
      "13           423.648685        600.480811            1967.595642   \n",
      "14           423.423049        600.538902            1968.905541   \n",
      "\n",
      "    BUILDING_Belt #2 - 길이  BUILDING_PA - 길이  \n",
      "0             1976.415917       1402.206362  \n",
      "1             1976.793411       1402.232596  \n",
      "2             1978.856869       1402.178547  \n",
      "3             1977.916774       1402.157351  \n",
      "4             1977.770122       1402.158150  \n",
      "5             1975.696008       1402.138250  \n",
      "6             1974.477613       1402.184257  \n",
      "7             1976.297997       1402.161401  \n",
      "8             1977.872625       1402.202936  \n",
      "9             1973.910250       1402.220327  \n",
      "10            1975.890923       1402.118272  \n",
      "11            1974.934878       1402.171537  \n",
      "12            1974.514319       1402.075809  \n",
      "13            1977.105302       1402.231720  \n",
      "14            1976.470603       1402.185859  \n"
     ]
    }
   ],
   "source": [
    "# Renormalize a new value x using the same scaling parameters\n",
    "def renormalize_back(x_normalized):\n",
    "    return x_normalized * (df.max() - df.min() ) + df.min() \n",
    "\n",
    "\n",
    "outputs = pd.DataFrame(decoded_samples, columns = features)\n",
    "outputs = renormalize_back(outputs)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hkt",
   "language": "python",
   "name": "hkt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
